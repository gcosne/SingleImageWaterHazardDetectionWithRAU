{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Segmented data / Generate the txt file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "list_img = os.listdir('/network/tmp1/cosnegau/Dataset/images/flood_seg')\n",
    "list_masks = os.listdir('/network/tmp1/cosnegau/Dataset/masks/flood_seg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/network/home/cosnegau/WaterDetection/source\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import TensorflowUtils as utils\n",
    "import datetime\n",
    "from six.moves import xrange\n",
    "import glob\n",
    "from skimage import io, transform, color\n",
    "import glob\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "tf.flags.DEFINE_integer(\"batch_size\", \"1\", \"batch size for training\")\n",
    "tf.flags.DEFINE_string(\"logs_dir\", \"/network/tmp1/cosnegau/models/\", \"path to logs directory\")\n",
    "tf.flags.DEFINE_string(\"logs_dir_fine\", \"/network/tmp1/cosnegau/models/fine_tuned/\", \"path to fine logs directory\")\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"/network/tmp1/cosnegau/Dataset/\", \"path to dataset\")\n",
    "tf.flags.DEFINE_string(\"output_dir\", \"/network/tmp1/cosnegau/results/\", \"path of output\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", \"1e-6\", \"Learning rate for Adam Optimizer\")\n",
    "tf.flags.DEFINE_bool('debug', \"False\", \"Debug mode: True/ False\")\n",
    "tf.flags.DEFINE_string('mode', \"train\", \"Mode train/ test/ visualize\")\n",
    "\n",
    "MAX_ITERATION = int(1e5 + 1)\n",
    "NUM_OF_CLASSESS = 2\n",
    "batch_offset = 0\n",
    "IMAGE_SIZE_HEIGHT = 360\n",
    "IMAGE_SIZE_WIDTH = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_dataset_path():\n",
    "    p = np.genfromtxt(FLAGS.data_dir+'flood_train.txt',dtype='str')\n",
    "    return p\n",
    "\n",
    "def load_data(p, step):\n",
    "    imgs=[]\n",
    "    gt_imgs=[]\n",
    "    gt_imgs2=[]\n",
    "    for i in range(p.shape[0]):\n",
    "        fp = p[i,0]\n",
    "        fp_gt = p[i,1]\n",
    "        #print(\"Loading images: %s \\t %s\"%(fp, fp_gt))\n",
    "\n",
    "        img = io.imread(fp)\n",
    "        img = img[:,0:1280,:]\n",
    "        img = transform.resize(img, (IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH))\n",
    "        imgs.append(img)\n",
    "\n",
    "        gt_img = io.imread(fp_gt)\n",
    "        gt_img = transform.resize(gt_img, (IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH))\n",
    "    for r in range(gt_img.shape[0]):\n",
    "        for c in range(gt_img.shape[1]):\n",
    "            if gt_img[r,c] == 1:\n",
    "                gt_img[r,c] = 1\n",
    "    else:\n",
    "        gt_img[r,c] = 0\n",
    "        \n",
    "    gt_imgs.append(gt_img)\n",
    "\n",
    "    gt_img2 = io.imread(fp_gt)\n",
    "    gt_img2 = transform.resize(gt_img2, (IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH))\n",
    "    for r in range(gt_img2.shape[0]):\n",
    "        for c in range(gt_img2.shape[1]):\n",
    "            if gt_img2[r,c] == 1:\n",
    "                gt_img2[r,c] = 0\n",
    "            else:\n",
    "                gt_img2[r,c] = 1\n",
    "    gt_imgs2.append(gt_img2)\n",
    "\n",
    "    return np.asarray(imgs,np.float32), np.asarray(gt_imgs,np.int32), np.asarray(gt_imgs2, np.int32)\n",
    "\n",
    "def load_test_data(p):\n",
    "    imgs=[]\n",
    "    im = p\n",
    "    img = io.imread(im)\n",
    "    #print('img.shape', img.shape)\n",
    "    img = img[:,0:1280,:]\n",
    "    img = transform.resize(img, (IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH))\n",
    "    imgs.append(img)\n",
    "    return np.asarray(imgs, np.float32)\n",
    "\n",
    "def inference(image, keep_prob):\n",
    "\n",
    "    with tf.variable_scope(\"seg_inference\"):\n",
    "        W1_1 = utils.weight_variable([3, 3, 3, 64], name=\"W1_1\")\n",
    "        b1_1 = utils.bias_variable([64], name=\"b1_1\")\n",
    "        conv1_1 = utils.conv2d_basic(image, W1_1, b1_1)\n",
    "        relu1_1 = tf.nn.relu(conv1_1, name=\"relu1_1\")\n",
    "\n",
    "        W1_2 = utils.weight_variable([3, 3, 64, 64], name=\"W1_2\")\n",
    "        b1_2 = utils.bias_variable([64], name=\"b1_2\")\n",
    "        conv1_2 = utils.conv2d_basic(relu1_1, W1_2, b1_2)\n",
    "        relu1_2 = tf.nn.relu(conv1_2, name=\"relu1_2\")\n",
    "\n",
    "        ra_1, ra_1_small = utils.RA_unit(relu1_2, relu1_2.shape[1].value, relu1_2.shape[2].value, 16)\n",
    "        W_s1 = utils.weight_variable([3, 3, 64*(1+16), 64], name=\"W_s1\")\n",
    "        b_s1 = utils.bias_variable([64], name=\"b_s1\")\n",
    "        conv_s1 = utils.conv2d_basic(ra_1, W_s1, b_s1)\n",
    "        relu_s1 = tf.nn.relu(conv_s1, name=\"relu_s1\")\n",
    "\n",
    "        pool1 = utils.max_pool_2x2(relu_s1)\n",
    "\n",
    "        W2_1 = utils.weight_variable([3, 3, 64, 128], name=\"W2_1\")\n",
    "        b2_1 = utils.bias_variable([128], name=\"b2_1\")\n",
    "        conv2_1 = utils.conv2d_basic(pool1, W2_1, b2_1)\n",
    "        relu2_1 = tf.nn.relu(conv2_1, name=\"relu2_1\")\n",
    "\n",
    "        W2_2 = utils.weight_variable([3, 3, 128, 128], name=\"W2_2\")\n",
    "        b2_2 = utils.bias_variable([128], name=\"b2_2\")\n",
    "        conv2_2 = utils.conv2d_basic(relu2_1, W2_2, b2_2)\n",
    "        relu2_2 = tf.nn.relu(conv2_2, name=\"relu2_2\")\n",
    "\n",
    "        ra_2, ra_2_small = utils.RA_unit(relu2_2, relu2_2.shape[1].value, relu2_2.shape[2].value, 16)\n",
    "        W_s2 = utils.weight_variable([3, 3, 128*(1+16), 128], name=\"W_s2\")\n",
    "        b_s2 = utils.bias_variable([128], name=\"b_s2\")\n",
    "        conv_s2 = utils.conv2d_basic(ra_2, W_s2, b_s2)\n",
    "        relu_s2 = tf.nn.relu(conv_s2, name=\"relu_s2\")\n",
    "\n",
    "        pool2 = utils.max_pool_2x2(relu_s2)\n",
    "\n",
    "        W3_1 = utils.weight_variable([3, 3, 128, 256], name=\"W3_1\")\n",
    "        b3_1 = utils.bias_variable([256], name=\"b3_1\")\n",
    "        conv3_1 = utils.conv2d_basic(pool2, W3_1, b3_1)\n",
    "        relu3_1 = tf.nn.relu(conv3_1, name=\"relu3_1\")\n",
    "\n",
    "        W3_2 = utils.weight_variable([3, 3, 256, 256], name=\"W3_2\")\n",
    "        b3_2 = utils.bias_variable([256], name=\"b3_2\")\n",
    "        conv3_2 = utils.conv2d_basic(relu3_1, W3_2, b3_2)\n",
    "        relu3_2 = tf.nn.relu(conv3_2, name=\"relu3_2\")\n",
    "\n",
    "        W3_3 = utils.weight_variable([3, 3, 256, 256], name=\"W3_3\")\n",
    "        b3_3 = utils.bias_variable([256], name=\"b3_3\")\n",
    "        conv3_3 = utils.conv2d_basic(relu3_2, W3_3, b3_3)\n",
    "        relu3_3 = tf.nn.relu(conv3_3, name=\"relu3_3\")\n",
    "\n",
    "        ra_3, ra_3_small = utils.RA_unit(relu3_3, relu3_3.shape[1].value, relu3_3.shape[2].value, 16)\n",
    "        W_s3 = utils.weight_variable([3, 3, 256*(1+16), 256], name=\"W_s3\")\n",
    "        b_s3 = utils.bias_variable([256], name=\"b_s3\")\n",
    "        conv_s3 = utils.conv2d_basic(ra_3, W_s3, b_s3)\n",
    "        relu_s3 = tf.nn.relu(conv_s3, name=\"relu_s3\")\n",
    "\n",
    "        pool3 = utils.max_pool_2x2(relu_s3)\n",
    "\n",
    "        W4_1 = utils.weight_variable([3, 3, 256, 512], name=\"W4_1\")\n",
    "        b4_1 = utils.bias_variable([512], name=\"b4_1\")\n",
    "        conv4_1 = utils.conv2d_basic(pool3, W4_1, b4_1)\n",
    "        relu4_1 = tf.nn.relu(conv4_1, name=\"relu4_1\")\n",
    "\n",
    "        W4_2 = utils.weight_variable([3, 3, 512, 512], name=\"W4_2\")\n",
    "        b4_2 = utils.bias_variable([512], name=\"b4_2\")\n",
    "        conv4_2 = utils.conv2d_basic(relu4_1, W4_2, b4_2)\n",
    "        relu4_2 = tf.nn.relu(conv4_2, name=\"relu4_2\")\n",
    "\n",
    "        W4_3 = utils.weight_variable([3, 3, 512, 512], name=\"W4_3\")\n",
    "        b4_3 = utils.bias_variable([512], name=\"b4_3\")\n",
    "        conv4_3 = utils.conv2d_basic(relu4_2, W4_3, b4_3)\n",
    "        relu4_3 = tf.nn.relu(conv4_3, name=\"relu4_3\")\n",
    "\n",
    "        ra_4, ra_4_small = utils.RA_unit(relu4_3, relu4_3.shape[1].value, relu4_3.shape[2].value, 16)\n",
    "        W_s4 = utils.weight_variable([3, 3, 512*(1+16), 512], name=\"W_s4\")\n",
    "        b_s4 = utils.bias_variable([512], name=\"b_s4\")\n",
    "        conv_s4 = utils.conv2d_basic(ra_4, W_s4, b_s4)\n",
    "        relu_s4 = tf.nn.relu(conv_s4, name=\"relu_s4\")\n",
    "\n",
    "        pool4 = utils.max_pool_2x2(relu_s4)\n",
    "\n",
    "        W5_1 = utils.weight_variable([3, 3, 512, 512], name=\"W5_1\")\n",
    "        b5_1 = utils.bias_variable([512], name=\"b5_1\")\n",
    "        conv5_1 = utils.conv2d_basic(pool4, W5_1, b5_1)\n",
    "        relu5_1 = tf.nn.relu(conv5_1, name=\"relu5_1\")\n",
    "\n",
    "        W5_2 = utils.weight_variable([3, 3, 512, 512], name=\"W5_2\")\n",
    "        b5_2 = utils.bias_variable([512], name=\"b5_2\")\n",
    "        conv5_2 = utils.conv2d_basic(relu5_1, W5_2, b5_2)\n",
    "        relu5_2 = tf.nn.relu(conv5_2, name=\"relu5_2\")\n",
    "\n",
    "        W5_3 = utils.weight_variable([3, 3, 512, 512], name=\"W5_3\")\n",
    "        b5_3 = utils.bias_variable([512], name=\"b5_3\")\n",
    "        conv5_3 = utils.conv2d_basic(relu5_2, W5_3, b5_3)\n",
    "        relu5_3 = tf.nn.relu(conv5_3, name=\"relu5_3\")\n",
    "\n",
    "        ra_5, ra_5_small = utils.RA_unit(relu5_3, relu5_3.shape[1].value, relu5_3.shape[2].value, 8)\n",
    "        W_s5 = utils.weight_variable([3, 3, 512*(1+8), 512], name=\"W_s5\")\n",
    "        b_s5 = utils.bias_variable([512], name=\"b_s5\")\n",
    "        conv_s5 = utils.conv2d_basic(ra_5, W_s5, b_s5)\n",
    "        relu_s5 = tf.nn.relu(conv_s5, name=\"relu_s5\")\n",
    "\n",
    "        pool5 = utils.max_pool_2x2(relu_s5)\n",
    "\n",
    "        W6 = utils.weight_variable([7, 7, pool4.shape[3].value, 4096], name=\"W6\")\n",
    "        b6 = utils.bias_variable([4096], name=\"b6\")\n",
    "        conv6 = utils.conv2d_basic(pool4, W6, b6)\n",
    "        relu6 = tf.nn.relu(conv6, name=\"relu6\")\n",
    "\n",
    "        relu_dropout6 = tf.nn.dropout(relu6, keep_prob=keep_prob)\n",
    "\n",
    "        W7 = utils.weight_variable([1, 1, 4096, 4096], name=\"W7\")\n",
    "        b7 = utils.bias_variable([4096], name=\"b7\")\n",
    "        conv7 = utils.conv2d_basic(relu_dropout6, W7, b7)\n",
    "        relu7 = tf.nn.relu(conv7, name=\"relu7\")\n",
    "\n",
    "        relu_dropout7 = tf.nn.dropout(relu7, keep_prob=keep_prob)\n",
    "\n",
    "        W8 = utils.weight_variable([1, 1, 4096, NUM_OF_CLASSESS], name=\"W8\")            #in our case num_of_classess = 2 : road, non-road\n",
    "        b8 = utils.bias_variable([NUM_OF_CLASSESS], name=\"b8\")\n",
    "        conv8 = utils.conv2d_basic(relu_dropout7, W8, b8)\n",
    "\n",
    "        # now to upscale to actual image size\n",
    "        deconv_shape1 = pool3.get_shape()\n",
    "        W_t1 = utils.weight_variable([4, 4, deconv_shape1[3].value, NUM_OF_CLASSESS], name=\"W_t1\")\n",
    "        b_t1 = utils.bias_variable([deconv_shape1[3].value], name=\"b_t1\")\n",
    "        conv_t1 = utils.conv2d_transpose_strided(conv8, W_t1, b_t1, output_shape=tf.shape(pool3))\n",
    "        fuse_1 = tf.add(conv_t1, pool3, name=\"fuse_1\")\n",
    "\n",
    "        deconv_shape2 = pool2.get_shape()\n",
    "        W_t2 = utils.weight_variable([4, 4, deconv_shape2[3].value, deconv_shape1[3].value], name=\"W_t2\")\n",
    "        b_t2 = utils.bias_variable([deconv_shape2[3].value], name=\"b_t2\")\n",
    "        conv_t2 = utils.conv2d_transpose_strided(fuse_1, W_t2, b_t2, output_shape=tf.shape(pool2))\n",
    "        fuse_2 = tf.add(conv_t2, pool2, name=\"fuse_2\")\n",
    "        #print(\"fuse_2 shape:\")\n",
    "        #print(fuse_2.shape)\n",
    "\n",
    "        shape = tf.shape(image)\n",
    "        deconv_shape3 = tf.stack([shape[0], shape[1], shape[2], NUM_OF_CLASSESS])\n",
    "        W_t3 = utils.weight_variable([16, 16, NUM_OF_CLASSESS, fuse_2.shape[3].value], name=\"W_t3\")\n",
    "        b_t3 = utils.bias_variable([NUM_OF_CLASSESS], name=\"b_t3\")\n",
    "        conv_t3 = utils.conv2d_transpose_strided(fuse_2, W_t3, b_t3, output_shape=deconv_shape3, stride=4, stride_y=4)\n",
    "\n",
    "        annotation_pred = tf.argmax(conv_t3, dimension=3, name=\"prediction\")\n",
    "\n",
    "    return annotation_pred, conv_t3                # conv_t3 is the final result\n",
    "\n",
    "def train(loss_val, var_list):\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "    grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n",
    "    return optimizer.apply_gradients(grads)\n",
    "\n",
    "def next_batch(batch_size, step):\n",
    "    global batch_offset\n",
    "    global p\n",
    "    start = batch_offset\n",
    "    batch_offset += batch_size\n",
    "    if batch_offset > p.shape[0]:\n",
    "        #         print(\"Shuffle data\")\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(p.shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        p = p[perm]\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        batch_offset = batch_size\n",
    "    end = batch_offset\n",
    "    #print(\"train_img start %d end %d\"%(start, end))\n",
    "    return p[start:end]\n",
    "\n",
    "def main(argv=None):\n",
    "    keep_probability = tf.placeholder(tf.float32, name=\"keep_probabilty\")\n",
    "    image = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH, 3], name=\"input_image\")\n",
    "    annotation = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH, 2], name=\"annotation\")\n",
    "    pred_annotation, logits = inference(image, keep_probability)            # build the FCN graph\n",
    "\n",
    "    prob = tf.nn.softmax(logits)\n",
    "\n",
    "    logits_ = tf.reshape(logits, [1, IMAGE_SIZE_HEIGHT*IMAGE_SIZE_WIDTH, 2])\n",
    "    annotation_ = tf.reshape(annotation, [1, IMAGE_SIZE_HEIGHT*IMAGE_SIZE_WIDTH, 2])\n",
    "    loss = utils.focal_loss(logits_, annotation_)\n",
    "\n",
    "    trainable_var = tf.trainable_variables()\n",
    "    train_op = train(loss, trainable_var)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "\n",
    "    sess = tf.Session(config = config)\n",
    "\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.logs_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored...\")\n",
    "\n",
    "    if FLAGS.mode == \"train\":\n",
    "        print(\"Loading data\")\n",
    "        global p;\n",
    "        p = load_training_dataset_path()\n",
    "        print(\"Start Training...\")\n",
    "        for itr in xrange(MAX_ITERATION):\n",
    "            print('Step: %d'%(itr))\n",
    "            p1 = next_batch(FLAGS.batch_size, itr)\n",
    "            # print(\"p1:\\n\",p1)\n",
    "            x_train, y_train, y_train2 = load_data(p1, itr)\n",
    "            y_train = np.expand_dims(y_train, axis=3)\n",
    "            y_train2 = np.expand_dims(y_train2, axis=3)\n",
    "            label_in = np.concatenate((y_train2, y_train), axis=3)\n",
    "            #x_train, y_train = next_batch(train_imgs, gt_imgs, FLAGS.batch_size)    \n",
    "            feed_dict = {image: x_train, annotation: label_in, keep_probability: 0.85}\n",
    "            # print(\"train feed_dict done!\")\n",
    "            start_time = time.time()\n",
    "            sess.run(train_op, feed_dict = feed_dict)\n",
    "            dur = time.time() - start_time\n",
    "            print(\"duration : %f\"% dur)\n",
    "            if itr % 1 == 0:\n",
    "                train_loss = sess.run(loss, feed_dict = feed_dict)\n",
    "                print('KITTI Step: %d, Train_loss:%g'%(itr, train_loss))\n",
    "            if itr % 5000 == 0:\n",
    "                print('Save Net Model...')\n",
    "                saver.save(sess, FLAGS.logs_fine_dir + \"model.ckpt\", itr)\n",
    "            if itr % 5000 == 0 and itr >= 20000:\n",
    "                FLAGS.learning_rate = FLAGS.learning_rate / 2\n",
    "    elif FLAGS.mode == \"visualize\":\n",
    "        p = np.genfromtxt('../Dataset/on_road_test.txt', dtype='str')\n",
    "        for idx in range(0,p.shape[0]):\n",
    "            test_images1 = load_test_data(p[idx,0])\n",
    "            start_time = time.time()\n",
    "            likelyhood, pred = sess.run([prob, pred_annotation], feed_dict={image:test_images1, keep_probability: 1.0})\n",
    "            dur = time.time() - start_time\n",
    "            print(\"dur = \",dur)\n",
    "            #np.save('./likelyhood/test/likelyhood_%06d'%(idx), likelyhood)\n",
    "            print(pred.shape)\n",
    "            for itr in range(pred.shape[0]):\n",
    "                utils.save_image(pred[itr].astype(np.float32), FLAGS.output_dir, name=\"pred_test_bin_%06d\"%(idx))\n",
    "                print(\"Saved image :%d\"%(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /network/tmp1/cosnegau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
